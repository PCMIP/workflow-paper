---
title: "Workflow Paper"
author: "Simon Goring"
date: "February 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Submission

### Open Quaternary

*Methods papers that outline and test new techniques and discuss potential applications and significance of the technique. In addition, papers that critique or modify extant methodologies and approaches are welcome. Authors should provide a detailed summary of the protocol followed and establish replicability within the body of the paper.*

Submissions should be between 6,000 and 8,000 words in length for Research, Methods, and Review papers, although submissions of all lengths will be considered.

## Abstract


## Introduction

A number of papers use multiple models to reconstruct past climates from pollen, using a number of methods.  These methods are described in Chevalier &al.

Some background on reconstruction papers?

This benchmarking is neccessary because it provides a common platform under wehich methods can be tested to a set of pre-defined standards.  This then allows us to test hypotheses about different methods, examine the causes of method divergence and convergence and ask questions about how underlying physical and ecological processes might affect model reliability, precision and accuracy in a paleoecological context.

## Methods

Broad Outline

### Datasets
The benchmarking in this paper focuses on modern pollen samples such that both the assemblage and the environmental covariates of interest are both known.

We use XXX samples from North America and ... ... ... 

### Models

We evaluate the performance of X models for predicting a climate covariate of interest from pollen assemblages: modern analog technique (MAT), weighted averaging (WA), maximum likelihood response curves (MLRC), random forest (RF), ... ... 

For a description of these models and relevant references the reader is referred to Chevalier, et al. (2018). 

Our benchmarking framework has been developed to easily include any other models that future researchers may want to evaluate. 

### Tests
Cross-validation is a common method in statistics and machine learning to evaluate the predictive performance of models. In cross-validation experiments the covariate of interest is known but held out for a subset of the data (called a test set). The models to be evaluated are then fit on the rest of the data (called a training set). Predictions are then generated for the test set. Those predictions are then evaluted against the known truth. The whole process is then repeated with a different test set until every observation has been in the test set exactly once.   

Here we perform 10-fold cross-validation, meaning each test set consists of 10% of the full training dataset for each of the 10 cross-validation folds. 

We evaluate the predictions for the covariate from cross-validation to the true covariate using four metrics: mean square error (MSE), mean absolute error (MAE), emipirical coverage of the 95% credible interval, and continuous rank probability score (CRPS). MSE is a metric of prediction accuracy of the mean. MAE is a metric of prediction accuracy of the median. Coverage is a metric of the accuracy of the prediction uncertainty. CRPS is a combined metric of the accuracy of both the mean and the spread of the predictions (Gneiting 2011). 

## Working Example

Climate datasets

### Results

## Conclusion

