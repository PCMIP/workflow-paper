---
title: "Workflow Paper"
author: "Simon Goring"
date: "February 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Submission

### Open Quaternary

*Methods papers that outline and test new techniques and discuss potential applications and significance of the technique. In addition, papers that critique or modify extant methodologies and approaches are welcome. Authors should provide a detailed summary of the protocol followed and establish replicability within the body of the paper.*

Submissions should be between 6,000 and 8,000 words in length for Research, Methods, and Review papers, although submissions of all lengths will be considered.

## Abstract


## Introduction

A number of papers use multiple models to reconstruct past climates from pollen, using a number of methods.  These methods are described in Chevalier &al.

Some background on reconstruction papers?

This benchmarking is neccessary because it provides a common platform under wehich methods can be tested to a set of pre-defined standards.  This then allows us to test hypotheses about different methods, examine the causes of method divergence and convergence and ask questions about how underlying physical and ecological processes might affect model reliability, precision and accuracy in a paleoecological context.

## Methods

Broad Outline

### Datasets

### Methods

### Tests
Cross-validation consists of five steps: 1) setting aside a subset of the data (called a test set), 2) fitting a model using the rest of the data (called a training set), 3) generating predictions for the held out test set, 4) evaluating the predictions versus the known truth for the test set, and 5) repeating the process with a different test and training set.

Testing models in cross-validation is a common method in statistics and machine learning to evaluate the predictive performance of a model

Here we perform 10-fold cross-validation, meaning the test set represents 10% of the full training dataset for each cross-validation fold. 

We evaluate the predictions using four metrics: mean square error (MSE), mean absolute error (MAE), emipirical coverage of the 95% credible interval, and continuous rank probability score (CRPS). MSE is a metric for prediction accuracy of the mean. MAE is a metric for prediction accuracy of the median. Coverage is a metric of the accuracy of the prediction uncertainty. CRPS is a combined metric for the accuracy of both the mean and the spread of the uncertainty (Gneiting 2011). 

## Working Example

Climate datasets

### Results

## Conclusion

