---
title: "Workflow Paper"
author: "Simon Goring"
date: "February 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```

## Submission

### Open Quaternary

*Methods papers that outline and test new techniques and discuss potential applications and significance of the technique. In addition, papers that critique or modify extant methodologies and approaches are welcome. Authors should provide a detailed summary of the protocol followed and establish replicability within the body of the paper.*

Submissions should be between 6,000 and 8,000 words in length for Research, Methods, and Review papers, although submissions of all lengths will be considered.

## Abstract


## Introduction

A number of papers use multiple models to reconstruct past climates from pollen, using a number of methods.  These methods are described in Chevalier &al.

Some background on reconstruction papers?

This benchmarking is neccessary because it provides a common platform under wehich methods can be tested to a set of pre-defined standards.  This then allows us to test hypotheses about different methods, examine the causes of method divergence and convergence and ask questions about how underlying physical and ecological processes might affect model reliability, precision and accuracy in a paleoecological context.

## Methods

Broad Outline

### Datasets


### Methods

### Tests
Cross-validation consists of five steps: 1) setting aside a subset of the data (called a test set), 2) fitting a model using the rest of the data (called a training set), 3) generating predictions for the held out test set, 4) evaluating the predictions versus the known truth for the test set, and 5) repeating the process with a different test and training set.

Testing models in cross-validation is a common method in statistics and machine learning to evaluate the predictive performance of a model

Here we perform 10-fold cross-validation, meaning the test set represents 10% of the full training dataset for each cross-validation fold. 

We evaluate the predictions using four metrics: mean square error (MSE), mean absolute error (MAE), emipirical coverage of the 95% credible interval, and continuous rank probability score (CRPS). MSE is a metric for prediction accuracy of the mean. MAE is a metric for prediction accuracy of the median. Coverage is a metric of the accuracy of the prediction uncertainty. CRPS is a combined metric for the accuracy of both the mean and the spread of the uncertainty (Gneiting 2011). 

## Working Example

A challenge for comparing climate reconstructions across publications has been the use of various climate base products to obtain climate data for calibration and, ultimately, reconstruction.  While researchers often base their choice of climate model on what is percieved to be an assessment of the objective quality of the underlying data, little is known about the effects of the choice of underlying climate data, or the resolution of that data on model performance.  REFER TO THE LADD PAPER HERE.

Among recent publications the breakdown of climate datasets is as follows:

| Climate Dataset |  Number of Papers |
| --------------- | ----------------- |
| PRISM           |        XXXX       |
| WorldClim       |        XXXX       |
| CRU (New et al) |        XXXX       |


The benchmarking method proposed above provides an opportunity to examine the impacts of base data choice and resolution, by providing a framework with which to test multiple pollen based climate models, multiple base climate data and multiple resolutions of the climate data.

```{r}

```
### Results

#### Differences by Resolution

```{r}
clim_files <- list.files('../benchmark-data/Pollen/Whitmore/',
                         pattern="worldclim", full.names = TRUE)

climate <- clim_files %>% 
  purrr::map(readr::read_csv) %>% 
  purrr::reduce(left_join, 
                by=c('X1', 'SITENAME', 'LONDD', 'LATDD', 'ELEVATION'))

```

## Conclusion

